{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9829c4a1",
   "metadata": {},
   "source": [
    "# Cleaning References pre-1970s\n",
    "The purpose of this notebook is to clean results obtained from mturk for journal articles that are from, on average, before 1970. There are two data sources which has resulted in slight differences in the raw data input structure.\n",
    "\n",
    "1. AWS MTURK - a service offered by AWS, output returns as a csv file\n",
    "2. fMTURK - a clone of AWS MTURK specific to scholarly publishing where the output returns as a json file\n",
    "\n",
    "Note that naming conventions for variables vary even though they are both structured data sets so combining them will require some trivial manipulation.\n",
    "\n",
    "Expected output: \n",
    "1. json files of reference matches \n",
    "2. json and csv files of references collected via manual interfaces\n",
    "3. csv file of all input data\n",
    "4. reconciliation of all input files vs output files to see which pages and files have been digitized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63915440",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d13f51e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries required, please install pandas\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from datetime import date\n",
    "import json\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "# set column options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46cab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change base path to point to the results of the mturk data\n",
    "# the expectation is that this was directly downloaded from the respective results interface\n",
    "mturk_files_out=\"/Users/sijiawu/Downloads/thesis_docs/mturk_process/output_files/\"\n",
    "mturk_files_in=\"/Users/sijiawu/Downloads/thesis_docs/mturk_process/input_files/\"\n",
    "fmturk_files_out=\"/Users/sijiawu/Downloads/thesis_docs/fmturk/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcd30924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/5mt219qj6l552yrf3l89xgdh0000gn/T/ipykernel_94468/2341136002.py:6: FutureWarning: Inferring datetime64[ns] from data containing strings is deprecated and will be removed in a future version. To retain the old behavior explicitly pass Series(data, dtype=datetime64[ns])\n",
      "  j_data=pd.concat([pd.read_excel('/Users/sijiawu/Work/Thesis/Data/Combined/'+i+'_M_sco_du.xlsx'), j_data], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# load in journal metadata\n",
    "JOURNALS= ['AER', 'JPE', 'ECTA', 'RES', 'QJE']\n",
    "#read in all processed masterlists\n",
    "j_data=pd.DataFrame()\n",
    "for i in JOURNALS:\n",
    "    j_data=pd.concat([pd.read_excel('/Users/sijiawu/Work/Thesis/Data/Combined/'+i+'_M_sco_du.xlsx'), j_data], ignore_index=True)\n",
    "#Create a batch file\n",
    "\n",
    "j_data=j_data[j_data.duplicated()==False].reset_index().drop('index', axis=1)\n",
    "\n",
    "# Replace the journal names with Acronyms\n",
    "j_data.loc[j_data['journal']==\"Econometrica\",'journal']='econometrica'\n",
    "j_data.loc[j_data['journal']=='The Quarterly Journal of Economics','journal']='quarterly journal of economics'\n",
    "j_data.loc[j_data['journal']=='The Review of Economic Studies','journal']='review of economic studies'\n",
    "j_data.loc[j_data['journal']=='Journal of Political Economy','journal']='journal of political economy'\n",
    "j_data.loc[j_data['journal']=='The American Economic Review','journal']='american economic review'\n",
    "\n",
    "#some corrections to the issue\n",
    "j_data.loc[j_data[\"number\"]==\"2023-03-04 00:00:00\",\"number\"]=\"3--4\"\n",
    "j_data.loc[j_data[\"number\"]==\"4-5\",\"number\"]=\"4--5\"\n",
    "j_data.loc[j_data[\"number\"]==\"1-2\",\"number\"]=\"1--2\"\n",
    "\n",
    "j_data.journal.unique()\n",
    "\n",
    "j_data[\"id\"]=j_data[\"URL\"].str.split(\"/\").str[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8111e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in mturk based files\n",
    "ref_mturk = [f for f in listdir(mturk_files_out) if isfile(join(mturk_files_out, f))]\n",
    "input_fs= [f for f in listdir(mturk_files_in) if isfile(join(mturk_files_in, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f9c87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file for recoding the column names on mturk retrieved data. I had two batches that had differing naming conventions. This ensures consistency.\n",
    "with open('./031_recon/recoding.json') as json_data:\n",
    "    dict = json.load(json_data)\n",
    "    json_data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23c9c495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fmturk data which is in json format.\n",
    "with open('./031_recon/response_1725501854297.json', 'r') as file:\n",
    "    ref_fmturk = json.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f73c01",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2b00b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove leading and trailing non-ascii characters\n",
    "def strip_leading(_str):\n",
    "    k=0\n",
    "    l=len(_str)\n",
    "    while k!=len(_str):\n",
    "        if re.search('[,*\" \\'.:]',_str[k]) is not None:\n",
    "            k=k+1\n",
    "        else:\n",
    "            break\n",
    "    while l>0:\n",
    "        if re.search('[,*\" \\'.:]',_str[l-1]) is not None:\n",
    "            l=l-1\n",
    "        else:\n",
    "            break\n",
    "    return _str[k:l]\n",
    "\n",
    "#reformat some spelling\n",
    "def replace_d_space(_str):\n",
    "    temp=re.sub(\"[ ]+\",\" \",_str)\n",
    "    temp=re.sub(\"ze \",\"se \",temp)\n",
    "    temp=re.sub(\"zation\",\"sation\",temp)\n",
    "    temp=re.sub(\"- | -\",\"-\", temp)\n",
    "    temp=re.sub(\"- | -\",\"-\", temp)\n",
    "    return temp\n",
    "\n",
    "# this function:\n",
    "# - removes all leading and trailing non-alphabet characters, in case there is some sort of punctuation copied\n",
    "# - remove + signs\n",
    "# - strips leading \"the\" if the second field is set to True\n",
    "def strip_and_convert(str_, strip_the):\n",
    "    #print(str_)\n",
    "    if pd.isna(str_)==True:\n",
    "        return \"None\"\n",
    "    temp=unidecode(str_)\n",
    "    try:\n",
    "        l = [x.isalpha() for x in temp].index(True)\n",
    "        m = [x.isalpha() for x in temp[::-1]].index(True)\n",
    "        temp=temp[l:len(str_)-m]\n",
    "    except:\n",
    "        print(temp)\n",
    "        temp=\"none\"\n",
    "    #temp=re.sub('^\"(.*)\"$','(.*)', temp)\n",
    "    if (temp[0:4].lower()==\"the \")&(strip_the==True):\n",
    "        temp=temp[4:]\n",
    "    temp=re.sub(' +', ' ', temp)\n",
    "    temp=re.sub(' ,', ', ', temp)\n",
    "    temp=temp.strip()\n",
    "    return temp\n",
    "\n",
    "roman_numerals = {\"I\" : 1,\n",
    "                  \"V\" : 5,\n",
    "                  \"X\" : 10,\n",
    "                  \"L\" : 50,\n",
    "                  \"C\" : 100,\n",
    "                  \"D\" : 500,\n",
    "                  \"M\" : 1000\n",
    "                  }\n",
    "\n",
    "def rom_to_dec(user_input):\n",
    "    int_value = 0\n",
    "    for i in range(len(user_input)):\n",
    "        if user_input[i] in roman_numerals:\n",
    "            if i + 1 < len(user_input) and roman_numerals[user_input[i]] < roman_numerals[user_input[i + 1]]:\n",
    "                int_value -= roman_numerals[user_input[i]]\n",
    "            else:\n",
    "                int_value += roman_numerals[user_input[i]]\n",
    "        else:\n",
    "            print(\"Invalid input.\")\n",
    "            return \"none\"\n",
    "\n",
    "    return int_value\n",
    "\n",
    "def rom_match(strg, search=re.compile(r'[^IVXLCDM]').search):\n",
    "     return not bool(search(strg))\n",
    "    \n",
    "def number_match(strg, search=re.compile(r'[^0-9.]').search):\n",
    "     return not bool(search(strg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92b9b22",
   "metadata": {},
   "source": [
    "## Formatting fMturk data\n",
    "\n",
    "Each page can have 0 to many references, this process flattens the json data into a pandas dataframe such that there is one reference per row. I also take the opportunity to add in any additional information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79179e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '665a9036e7af08e1933ac1f8',\n",
       " 'batchId': 'qje_remnant_b523',\n",
       " 'tasknum': 0,\n",
       " 'pdf_url': 'https://myawsbucket-1231.s3.eu-west-3.amazonaws.com/QJE_shards/1880567_wo_cover_page-8.pdf',\n",
       " 'issue': 'unavailable',\n",
       " 'volume': 'unavailable',\n",
       " 'journal': 'quarterly journal of economics',\n",
       " 'year': 1972,\n",
       " 'author_name': 'unavailable',\n",
       " 'answer': '[{\"id\":1,\"jstor\":\"\",\"type\":2,\"author\":\"D. Jorgenson and J. Stephenson\",\"title\":\"The Time Structure of Investment  Behavior in United States Manufacturing, 1947-1960\",\"journal\":\"Review of Economics and Statistics\",\"volume\":\"XLIX\",\"issue\":\"Feb\",\"year\":\"1967\",\"pages\":\"none\"},{\"id\":2,\"jstor\":\"1813210\",\"type\":8},{\"id\":3,\"jstor\":\"\",\"type\":2,\"author\":\"none\",\"title\":\"Investment Behavior and Neo-classical Theory\",\"journal\":\"Review of Economics and Statistics\",\"volume\":\"L\",\"issue\":\"Aug\",\"year\":\"1968\",\"pages\":\"none\"},{\"id\":4,\"jstor\":\"\",\"type\":2,\"author\":\"A Alchian\",\"title\":\"Information costs, pricing, and resource unemployment\",\"journal\":\"western economic journal\",\"volume\":\"VII\",\"issue\":\"June\",\"year\":\"1969\",\"pages\":\"none\"}]',\n",
       " 'complete': True,\n",
       " 'completer': '665123709458209e9841956e',\n",
       " 'audit': True}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_fmturk[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0911df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers={}\n",
    "issues_tasks=[]\n",
    "a=0\n",
    "for i in range(len(ref_fmturk)):\n",
    "    temp={\n",
    "        'tasknum':ref_fmturk[i]['id'],\n",
    "        'pdf_url':ref_fmturk[i]['pdf_url'],\n",
    "        'id_o': ref_fmturk[i]['pdf_url'].split(\"/\")[-1].split('_')[0],\n",
    "        'page_o':ref_fmturk[i]['pdf_url'].split(\"/\")[-1].split('_')[3].split('-')[-1].split('.')[0],\n",
    "        'issue_o':None, \n",
    "        'title_o':None,\n",
    "        'volume_o':None,\n",
    "        'authors_o':None,\n",
    "        'journal_o':ref_fmturk[i]['journal'], \n",
    "        'year_o':ref_fmturk[i]['year'],\n",
    "        'completer': ref_fmturk[i]['completer']\n",
    "        }\n",
    "    try:\n",
    "        answer=json.loads(ref_fmturk[i]['answer'])\n",
    "    except:\n",
    "        issues_tasks.append(ref_fmturk[i]['id'])\n",
    "        continue\n",
    "    # print(i)\n",
    "    for j in range(len(answer)):\n",
    "        a=a+1\n",
    "        # print(j)\n",
    "        answers[a]=answer[j]|temp\n",
    "\n",
    "rem_refs=pd.DataFrame(answers).transpose()\n",
    "rem_refs=rem_refs.fillna(\"none\")\n",
    "rem_refs['journal']=rem_refs['journal'].str.strip().str.lower()\n",
    "rem_refs=rem_refs.replace(to_replace='  ', value=' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8b3333",
   "metadata": {},
   "source": [
    "### Reconciliation of Journal names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ab08083",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./031_recon/fmturk_recon.json', 'r', encoding='utf-8') as file:\n",
    "    recon=json.load(file)\n",
    "    file.close()\n",
    "\n",
    "recon_split={}\n",
    "\n",
    "for i in recon.keys():\n",
    "    for j in recon[i]:\n",
    "        recon_split[j]=i\n",
    "\n",
    "for i in rem_refs.index:\n",
    "    if rem_refs.loc[i,'journal'] in recon_split.keys():\n",
    "        rem_refs.loc[i,'journal_proc'] =recon_split[rem_refs.loc[i,'journal']]\n",
    "\n",
    "# order=list(rem_refs['journal'].unique())\n",
    "# order.sort\n",
    "\n",
    "# for i in order:\n",
    "#     print('\"'+i+'\",')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c115ba",
   "metadata": {},
   "source": [
    "### Reconciliation of years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21a7eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./031_recon/fmturk_year_recon.json', 'r', encoding='utf-8') as file:\n",
    "    recon=json.load(file)\n",
    "    file.close()\n",
    "\n",
    "recon_split={}\n",
    "\n",
    "for i in recon.keys():\n",
    "    for j in recon[i]:\n",
    "        recon_split[j]=i\n",
    "    \n",
    "year_pot=[]\n",
    "nones=['none','n.d.','n. d.']\n",
    "for i in rem_refs.index:\n",
    "    try:\n",
    "        if rem_refs.loc[i,'year']in nones:\n",
    "            rem_refs.loc[i,'year_proc'] =0\n",
    "        elif rem_refs.loc[i,'year']=='forthcoming':\n",
    "            rem_refs.loc[i,'year_proc'] =3\n",
    "        else:\n",
    "            temp=int(float(rem_refs.loc[i,'year'].strip()))\n",
    "            rem_refs.loc[i,'year_proc'] =temp\n",
    "    except:\n",
    "        rem_refs.loc[i,'year_proc'] =recon_split[rem_refs.loc[i,'year'].strip()]\n",
    "        # print('\"'+rem_refs.loc[i,'year']+'\",')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f350728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_split(x):\n",
    "    if x[0]==\"none\":\n",
    "        return 0\n",
    "    if x[0]==\"wrong\":\n",
    "        return 1\n",
    "    if x[0]==\"forthcoming\":\n",
    "        return 3\n",
    "    if x[0]==\"uncertain\":\n",
    "        return 4\n",
    "    if x[0]==\"various\":\n",
    "        return 4\n",
    "    if \",\" in x[0]:\n",
    "        return int(x[0].split(',')[-1])\n",
    "    else:\n",
    "        if len(x)==1:\n",
    "            return int(float(x[0]))\n",
    "        if len(x)==2:\n",
    "            return int(float(x[1]))\n",
    "    \n",
    "rem_refs[\"year_proc_split\"]=rem_refs[\"year_proc\"].astype(str)\n",
    "rem_refs[\"year_proc_split\"]=rem_refs[\"year_proc_split\"].str.split('-')\n",
    "rem_refs[\"year_latest\"]=rem_refs[\"year_proc_split\"].apply(lambda x: year_split(x))\n",
    "rem_refs[\"year_latest\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501f8410",
   "metadata": {},
   "source": [
    "### Reconciliation of issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc9d5cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_refs[\"issue_proc\"]=rem_refs['issue'].str.lower().str.strip()\n",
    "\n",
    "#read in the title recon file\n",
    "issue_rec_f=None\n",
    "with open(\"./031_recon/fmturk_issue_recon.json\", 'r') as f:\n",
    "    issue_rec_f=json.load(f) \n",
    "\n",
    "#reformat and expand the title recon file\n",
    "issue_rec_f_split_out={}\n",
    "for key in issue_rec_f.keys():\n",
    "    for k in issue_rec_f[key]:\n",
    "        issue_rec_f_split_out[k]=key  \n",
    "\n",
    "issue_corr=[]\n",
    "w_count=0\n",
    "for i in rem_refs.index:\n",
    "    val=rem_refs.loc[i,'issue_proc']\n",
    "    ind=0\n",
    "    if val in issue_rec_f_split_out.keys():\n",
    "        val=issue_rec_f_split_out[val]\n",
    "    if val in issue_rec_f_split_out.keys():\n",
    "        rem_refs.loc[i,'issue_proc']=val\n",
    "    #         print(val)\n",
    "    else:\n",
    "        w_count=w_count+1\n",
    "        issue_corr.append(val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9e2277",
   "metadata": {},
   "source": [
    "### Reconciliation of Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6012fce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_refs[\"volume_proc\"]=rem_refs['volume'].str.upper().str.strip()\n",
    "\n",
    "#read in the title recon file\n",
    "volume_rec_f=None\n",
    "with open(\"./031_recon/fmturk_volume_recon.json\", 'r') as f:\n",
    "    volume_rec_f=json.load(f) \n",
    "\n",
    "#reformat and expand the title recon file\n",
    "volume_rec_f_split_out={}\n",
    "for key in volume_rec_f.keys():\n",
    "    for k in volume_rec_f[key]:\n",
    "        volume_rec_f_split_out[k]=key\n",
    "\n",
    "\n",
    "volume_corr=[]\n",
    "w_count=0\n",
    "for i in rem_refs.index:\n",
    "    val=rem_refs.loc[i,'volume_proc']\n",
    "    ind=0\n",
    "    if val in volume_rec_f_split_out.keys():\n",
    "        val=volume_rec_f_split_out[val]\n",
    "    if val in volume_rec_f_split_out.keys():\n",
    "        rem_refs.loc[i,'volume_proc']=val\n",
    "    #         print(val)\n",
    "    else:\n",
    "        w_count=w_count+1\n",
    "        volume_corr.append(val)\n",
    "# vol_rec_f=list(set(rem_refs[\"volume_proc\"]))\n",
    "# vol_rec_f.sort()\n",
    "# for i in vol_rec_f:\n",
    "#     print('\"'+i+'\":[\"'+i+'\"],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90e6792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbd7c560",
   "metadata": {},
   "source": [
    "### Title recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b53d209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2950"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pre-process the title with previous two functions\n",
    "rem_refs[\"title_proc\"]=rem_refs[\"title\"].fillna(\"none\").astype(str).str.lower()\n",
    "rem_refs[\"title_proc\"]=rem_refs[\"title_proc\"].apply(strip_leading,1)\n",
    "rem_refs[\"title_proc\"]=rem_refs[\"title_proc\"].apply(replace_d_space,1)\n",
    "title_sort=list(rem_refs.title_proc.unique())\n",
    "title_sort.sort()\n",
    "\n",
    "j_data[\"title_proc\"]=j_data[\"title\"].fillna(\"none\").astype(str).str.lower()\n",
    "j_data[\"title_proc\"]=j_data[\"title_proc\"].apply(strip_leading,1)\n",
    "\n",
    "len(title_sort) #total titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9126f02e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "328cdbbf",
   "metadata": {},
   "source": [
    "### Some statistics and data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4c08f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8    2547\n",
       "3    1710\n",
       "2    1328\n",
       "4     730\n",
       "1     139\n",
       "7     137\n",
       "6     115\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rem_refs['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d822f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_list_app=[\"american economic review\",\"econometrica\", \"journal of political economy\", \"quarterly journal of economics\", \"review of economic studies\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "463c1f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172, 30)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rem_refs[rem_refs['journal_proc'].isin(j_list_app)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cb6fd3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(issues_tasks) #list of tasks with issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9c94e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'jstor', 'type', 'author', 'title', 'journal', 'volume', 'issue',\n",
       "       'year', 'pages', 'tasknum', 'pdf_url', 'id_o', 'page_o', 'issue_o',\n",
       "       'title_o', 'volume_o', 'authors_o', 'journal_o', 'year_o', 'completer',\n",
       "       'publisher', 'location', 'chapter_title', 'text_full', 'journal_proc',\n",
       "       'year_proc', 'issue_proc', 'volume_proc', 'title_proc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rem_refs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27706b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>jstor</th>\n",
       "      <th>type</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>journal</th>\n",
       "      <th>volume</th>\n",
       "      <th>issue</th>\n",
       "      <th>year</th>\n",
       "      <th>pages</th>\n",
       "      <th>...</th>\n",
       "      <th>completer</th>\n",
       "      <th>publisher</th>\n",
       "      <th>location</th>\n",
       "      <th>chapter_title</th>\n",
       "      <th>text_full</th>\n",
       "      <th>journal_proc</th>\n",
       "      <th>year_proc</th>\n",
       "      <th>issue_proc</th>\n",
       "      <th>volume_proc</th>\n",
       "      <th>title_proc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>D. Jorgenson and J. Stephenson</td>\n",
       "      <td>The Time Structure of Investment Behavior in United States Manufacturing, 1947-1960</td>\n",
       "      <td>review of economics and statistics</td>\n",
       "      <td>XLIX</td>\n",
       "      <td>Feb</td>\n",
       "      <td>1967</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>665123709458209e9841956e</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>review of economics and statistics</td>\n",
       "      <td>1967.0</td>\n",
       "      <td>feb</td>\n",
       "      <td>XLIX</td>\n",
       "      <td>the time structure of investment behavior in united states manufacturing, 1947-1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1813210</td>\n",
       "      <td>8</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>665123709458209e9841956e</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>none</td>\n",
       "      <td>NONE</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>none</td>\n",
       "      <td>Investment Behavior and Neo-classical Theory</td>\n",
       "      <td>review of economics and statistics</td>\n",
       "      <td>L</td>\n",
       "      <td>Aug</td>\n",
       "      <td>1968</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>665123709458209e9841956e</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>review of economics and statistics</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>aug</td>\n",
       "      <td>L</td>\n",
       "      <td>investment behavior and neo-classical theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>A Alchian</td>\n",
       "      <td>Information costs, pricing, and resource unemployment</td>\n",
       "      <td>western economic journal</td>\n",
       "      <td>VII</td>\n",
       "      <td>June</td>\n",
       "      <td>1969</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>665123709458209e9841956e</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>june</td>\n",
       "      <td>VII</td>\n",
       "      <td>information costs, pricing, and resource unemployment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    jstor  type                          author  \\\n",
       "1   1              2  D. Jorgenson and J. Stephenson   \n",
       "2   2  1813210     8                            none   \n",
       "3   3              2                            none   \n",
       "4   4              2                       A Alchian   \n",
       "\n",
       "                                                                                 title  \\\n",
       "1  The Time Structure of Investment Behavior in United States Manufacturing, 1947-1960   \n",
       "2                                                                                 none   \n",
       "3                                         Investment Behavior and Neo-classical Theory   \n",
       "4                                Information costs, pricing, and resource unemployment   \n",
       "\n",
       "                              journal volume issue  year pages  ...  \\\n",
       "1  review of economics and statistics   XLIX   Feb  1967  none  ...   \n",
       "2                                none   none  none  none  none  ...   \n",
       "3  review of economics and statistics      L   Aug  1968  none  ...   \n",
       "4            western economic journal    VII  June  1969  none  ...   \n",
       "\n",
       "                  completer publisher location chapter_title text_full  \\\n",
       "1  665123709458209e9841956e      none     none          none      none   \n",
       "2  665123709458209e9841956e      none     none          none      none   \n",
       "3  665123709458209e9841956e      none     none          none      none   \n",
       "4  665123709458209e9841956e      none     none          none      none   \n",
       "\n",
       "                         journal_proc year_proc issue_proc volume_proc  \\\n",
       "1  review of economics and statistics    1967.0        feb        XLIX   \n",
       "2                                 NaN       0.0       none        NONE   \n",
       "3  review of economics and statistics    1968.0        aug           L   \n",
       "4                                 NaN    1969.0       june         VII   \n",
       "\n",
       "                                                                            title_proc  \n",
       "1  the time structure of investment behavior in united states manufacturing, 1947-1960  \n",
       "2                                                                                 none  \n",
       "3                                         investment behavior and neo-classical theory  \n",
       "4                                information costs, pricing, and resource unemployment  \n",
       "\n",
       "[4 rows x 30 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rem_refs.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fb0339",
   "metadata": {},
   "source": [
    "## Format and load Mturk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df79a71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23517, 582)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in each mturk file and rename the columns\n",
    "All=[]\n",
    "for i in ref_mturk:\n",
    "    All.append(pd.read_csv(mturk_files_out+i).rename(columns=dict))\n",
    "All=pd.concat(All, ignore_index=True)\n",
    "All.sort_index(axis=1, inplace=True)\n",
    "All.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a15e8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23517, 582)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for duplicates by checking that dataframe size is the same after dropping duplicates\n",
    "check=All.drop_duplicates()\n",
    "check.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db211e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(check[\"RejectionTime\"].isna()==False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa83b6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AER                           6974\n",
       "QJE                           1210\n",
       "JPE                             76\n",
       "ECONOMETRICA                    75\n",
       "Review of Economic Studies      32\n",
       "Name: journal, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check remainder\n",
    "Allin=[]\n",
    "p_url=list(All[All[\"RejectionTime\"].isna()!=False][\"Input.pdf_url\"]) # this does not include \n",
    "\n",
    "for i in input_fs:\n",
    "    Allin.append(pd.read_csv(mturk_files_in+i))\n",
    "\n",
    "Allin=pd.concat(Allin, ignore_index=True)\n",
    "Allin=Allin.drop_duplicates()\n",
    "\n",
    "# prep remainder for processing\n",
    "tmp=Allin[Allin[\"pdf_url\"].isin(p_url)==False].reset_index(drop=True)\n",
    "tmp[\"journal\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c9e7c",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "In this section, the mturk data is restructured such that each line has a single entered reference. Some unnecessary fields from the mturk data are also dropped. We keep the HITId to keep a foreign key linking back to the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fdfb7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the column names\n",
    "# list(All.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fe5919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns to drop\n",
    "lst=['HITTypeId',\n",
    " 'Title',\n",
    " 'Description',\n",
    " 'Keywords',\n",
    " 'Reward',\n",
    " 'CreationTime',\n",
    " 'MaxAssignments',\n",
    " 'RequesterAnnotation',\n",
    " 'AssignmentDurationInSeconds',\n",
    " 'AutoApprovalDelayInSeconds',\n",
    " 'Expiration',\n",
    " 'NumberOfSimilarHITs',\n",
    " 'LifetimeInSeconds',\n",
    " 'AssignmentId',\n",
    " 'AssignmentStatus',\n",
    " 'AcceptTime',\n",
    " 'AutoApprovalTime',\n",
    " 'ApprovalTime',\n",
    " 'RejectionTime',\n",
    " 'LifetimeApprovalRate',\n",
    " 'Last30DaysApprovalRate',\n",
    " 'Last7DaysApprovalRate',\n",
    " 'SubmitTime']\n",
    "\n",
    "All=All.drop(lst, axis=1)\n",
    "\n",
    "# add for number of referencing counts\n",
    "All[\"num_refs\"] = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcd4052",
   "metadata": {},
   "source": [
    "### Functions used to clean individual reference entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33dfa14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# the following three functions expect to recieve a row of the data from mturk and the number of the reference on the page.\n",
    "# after which it will return that reference as a json dictionary.\n",
    "# Please see sample entry from from Mturk to see the fields expected for each type of data entry.\n",
    "def process_article(x, num):\n",
    "    article_dict={\n",
    "        \"type\": 2,\n",
    "        \"author\": x[\"Answer.ref.\"+str(num)+\"_author\"],\n",
    "        \"title\": x[\"Answer.ref.\"+str(num)+\"_title\"],\n",
    "        \"journal\": strip_and_convert(x[\"Answer.ref.\"+str(num)+\"_journal\"], True),\n",
    "        \"year\": x[\"Answer.ref.\"+str(num)+\"_year\"],\n",
    "        \"volume\": x[\"Answer.ref.\"+str(num)+\"_vol\"],\n",
    "        \"issue\": x[\"Answer.ref.\"+str(num)+\"_issue\"],\n",
    "        \"pages\": x[\"Answer.ref.\"+str(num)+\"_pages\"],  \n",
    "    }\n",
    "    \n",
    "    return article_dict\n",
    "\n",
    "def process_book(x, num):\n",
    "    book_dict={\n",
    "        \"type\": 3,\n",
    "        \"author\": x[\"Answer.ref.\"+str(num)+\"_author\"],\n",
    "        \"title\": x[\"Answer.ref.\"+str(num)+\"_title\"],\n",
    "        \"chapter_title\": x[\"Answer.ref.\"+str(num)+\"_chapter_title\"],\n",
    "        \"year\": x[\"Answer.ref.\"+str(num)+\"_year\"],\n",
    "        \"volume\": x[\"Answer.ref.\"+str(num)+\"_vol\"],\n",
    "        \"location\": x[\"Answer.ref.\"+str(num)+\"_location\"],\n",
    "        \"publisher\": x[\"Answer.ref.\"+str(num)+\"_publisher\"],\n",
    "        \"pages\": x[\"Answer.ref.\"+str(num)+\"_pages\"],  \n",
    "    }\n",
    "    return book_dict\n",
    "\n",
    "def process_other(x, num):\n",
    "    other_dict={\n",
    "        \"type\": 4,\n",
    "        \"author\": x[\"Answer.ref.\"+str(num)+\"_author\"],\n",
    "        \"title\": x[\"Answer.ref.\"+str(num)+\"_title\"],\n",
    "        \"year\": x[\"Answer.ref.\"+str(num)+\"_year\"],\n",
    "        \"publisher\": x[\"Answer.ref.\"+str(num)+\"_publisher\"],\n",
    "        \"text_full\": x[\"Answer.ref.\"+str(num)+\"_textfull\"],  \n",
    "    }\n",
    "    return other_dict\n",
    "\n",
    "def process_news(x, num):\n",
    "    news_dict={\n",
    "        \"type\": 6,\n",
    "        \"year\": x[\"Answer.ref.\"+str(num)+\"_year\"],\n",
    "        \"publisher\": x[\"Answer.ref.\"+str(num)+\"_publisher\"],\n",
    "        \"text_full\": x[\"Answer.ref.\"+str(num)+\"_textfull\"],  \n",
    "    }\n",
    "    return news_dict\n",
    "\n",
    "def process_laws(x, num):\n",
    "    law_dict={\n",
    "        \"type\": 7,\n",
    "        \"year\": x[\"Answer.ref.\"+str(num)+\"_year\"],\n",
    "        \"text_full\": x[\"Answer.ref.\"+str(num)+\"_textfull\"],  \n",
    "    }\n",
    "    return law_dict\n",
    "\n",
    "def process_jstor(x, num):\n",
    "    jstor_dict={\n",
    "        \"type\": 8,\n",
    "        \"year\": x[\"Answer.ref.\"+str(num)+\"_year\"],\n",
    "        \"jstor\": x[\"Answer.jstor_\"+str(num)],  \n",
    "    }\n",
    "    return jstor_dict\n",
    "\n",
    "\n",
    "# function to merge dictionaries\n",
    "def Merge(dict1, dict2):\n",
    "    return(dict2|dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ba2a9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "a=0\n",
    "all_ref={}\n",
    "\n",
    "# Clean out each reference\n",
    "for j in All[All[\"RequesterFeedback\"].isna()!=False].index:\n",
    "    count=0\n",
    "    # print(a)\n",
    "    for i in range(0,30,1):\n",
    "        item=None\n",
    "        temp={\n",
    "            \"id\": i+1,\n",
    "            \"tasknum\":All.iloc[j][\"HITId\"],\n",
    "            \"id_o\": All.iloc[j][\"Input.ID\"],\n",
    "            \"page_o\": All.iloc[j][\"Input.page\"],\n",
    "            \"year_o\": All.iloc[j][\"Input.year\"],\n",
    "            \"journal_o\": All.iloc[j][\"Input.journal\"],\n",
    "            \"authors_o\": All.iloc[j][\"Input.authors\"],\n",
    "            \"title_o\": All.iloc[j][\"Input.title\"],\n",
    "            \"volume_o\": All.iloc[j][\"Input.vol\"],\n",
    "            \"issue_o\": All.iloc[j][\"Input.issue\"],\n",
    "            'completer': All.iloc[j]['WorkerId'],\n",
    "            'pdf_url':All.iloc[j]['Input.pdf_url']\n",
    "        }\n",
    "        \n",
    "        if pd.isna(All.iloc[j][\"Answer.\"+str(i)+\".1\"]):\n",
    "            continue\n",
    "        elif All.iloc[j][\"Answer.\"+str(i)+\".1\"]==True:\n",
    "            item={'type':1}\n",
    "        elif All.iloc[j][\"Answer.\"+str(i)+\".2\"]==True:\n",
    "            count=count+1\n",
    "            item=process_article(All.iloc[j],i)\n",
    "        elif All.iloc[j][\"Answer.\"+str(i)+\".3\"]==True:\n",
    "            count=count+1\n",
    "            item=process_book(All.iloc[j],i)\n",
    "        elif All.iloc[j][\"Answer.\"+str(i)+\".4\"]==True:\n",
    "            count=count+1\n",
    "            item=process_other(All.iloc[j],i)\n",
    "        elif All.iloc[j][\"Answer.\"+str(i)+\".5\"]==True:\n",
    "            item={'type':5}\n",
    "        elif All.iloc[j][\"Answer.\"+str(i)+\".6\"]==True:\n",
    "            item=process_news(All.iloc[j],i)\n",
    "            count=count+1\n",
    "        elif All.iloc[j][\"Answer.\"+str(i)+\".7\"]==True:\n",
    "            item=process_laws(All.iloc[j],i)\n",
    "            count=count+1\n",
    "        elif All.iloc[j][\"Answer.\"+str(i)+\".8\"]==True:\n",
    "            item=process_jstor(All.iloc[j],i)\n",
    "            count=count+1\n",
    "            \n",
    "        if item != None:\n",
    "            a=a+1\n",
    "            all_ref[a]=temp|item\n",
    "\n",
    "#NB change this into switch statement, horrible if ladder is good enough for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12603fd7",
   "metadata": {},
   "source": [
    "## Reconciliation of Metadata Fields\n",
    "\n",
    "This is to increase the percentage match on a metadata dield in the masterlist. The specific fields going through reconciliation are:\n",
    "- journal name\n",
    "- pages\n",
    "- volume\n",
    "- issue\n",
    "- year\n",
    "- title\n",
    "\n",
    "The output from above is a dictionary of the separated out responses in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a2368b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose the data into a dataframe\n",
    "ar=pd.DataFrame.from_dict(all_ref).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ed1585c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    13723\n",
       "2    12482\n",
       "4    11788\n",
       "1     6566\n",
       "8     1016\n",
       "7      856\n",
       "6      320\n",
       "5       23\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac187ace",
   "metadata": {},
   "source": [
    "## Reconciliating Journal names\n",
    "\n",
    "There are many misspellings of journal names, this section is correct them. Process:\n",
    "- Strip leading and trailing white spaces\n",
    "- Ensure \"the\" has been removed from the beginning of the journal, this was done during the previous step.\n",
    "- Lowercase the list, get a unique list and sort alphabetically\n",
    "- Go through the list and copy duplicates into an array with the key as the corrected spelling: eg \"AER\":[\"AERS\",\"AERB\" ...],\n",
    "- Format each misspelling into a dictionary in the form {misspelling: correction, ....,misspelling: correction} in preparation of replacing it in the dataset.\n",
    "- Create a separate column in the data that is a copy of the journal column, journal_proc, caste it to string type and replace the errors in this column.\n",
    "\n",
    "The result is a json file where each key is a journal name and an array of errors. And a json file where each error name maps to the corrected journal name including journal names that are not in error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bc8d222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1886"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_journals=list(ar[ar[\"type\"]==2][\"journal\"].str.strip().str.lower().unique())\n",
    "unique_journals.sort()\n",
    "len(unique_journals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "500f3f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the journal names from file\n",
    "sort_info=None\n",
    "with open(\"./031_recon/mturk_journal_name_recon.json\", 'r') as f:\n",
    "    sort_info = json.load(f)\n",
    "    \n",
    "# def convert(o):\n",
    "#     if isinstance(o, np.int64): return int(o)  \n",
    "#     raise TypeError\n",
    "    \n",
    "# sorted_list = sorted(sort_info.items())\n",
    "\n",
    "# sorted_dict = {}\n",
    "# for key, value in sorted_list:\n",
    "#     sorted_dict[key] = value\n",
    "\n",
    "# print(sorted_dict)\n",
    "# with open(\"journal_name_recon.json\", 'w') as f:\n",
    "#     json.dump(sorted_dict, f, indent = 6,default=convert) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1ade34",
   "metadata": {},
   "source": [
    "### Format each journal name error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "056748be",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_out={}\n",
    "\n",
    "for key in sort_info.keys():\n",
    "    for i in sort_info[key]:\n",
    "        split_out[i]=key\n",
    "\n",
    "for i in range(len(unique_journals)):\n",
    "    if unique_journals[i] in split_out.keys():\n",
    "        unique_journals[i]=split_out[unique_journals[i]]\n",
    "        \n",
    "for i in unique_journals:\n",
    "    if i not in split_out.keys():\n",
    "        split_out[i]=i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557930a4",
   "metadata": {},
   "source": [
    "### Replace the journal names for the ones we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d00056e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar[\"journal_proc\"]=ar['journal'].fillna(\"none\").astype(str).str.lower().str.strip()\n",
    "# joi=['american economic review', 'journal of political economy','econometrica','quarterly journal of economics', 'review of economic studies']\n",
    "for i in ar[ar[\"journal_proc\"].isna()==False].index:\n",
    "    if ar.loc[i,'journal_proc'] in split_out.keys():\n",
    "        ar.loc[i,'journal_proc']=split_out[ar.loc[i,'journal_proc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd86e68",
   "metadata": {},
   "source": [
    "## Reconcile the years\n",
    "1. fillna as \"none\" and convert the year column to type string, store it in a new column called year_proc\n",
    "2. some entered a month or season followed by a space and then the year, split by space or comma and take the last year value entered.\n",
    "3. for each value, try caste the string year to an int, either directly or first converted to a float\n",
    "4. append all other cases to a list called year_corr, get a unique set and reconcile manually via a list called year_rec\n",
    "5. Format of errors is {correct_year: [year_error1, year_error2 ...], ...}\n",
    "6. Save it to a file called year_recon.json. this is the file that is being read in in the next code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c94cf0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_rec=None\n",
    "with open(\"./031_recon/mturk_year_recon.json\", 'r') as f:\n",
    "    year_rec = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c11b8144",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_rec_split_out={}\n",
    "for key in year_rec.keys():\n",
    "    for i in year_rec[key]:\n",
    "        year_rec_split_out[i]=key\n",
    "# format the year_rec dictionary into one such that it is \"error\":\"correction\" form for each error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0be3141",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar[\"year_proc\"]=ar['year'].fillna(\"none\").astype(str)\n",
    "ar[\"year_proc\"]=ar[\"year_proc\"].str.strip().str.split(',| ').str[-1]\n",
    "\n",
    "year_corr=[] #this contains anything that isn't resolved in year_rec\n",
    "\n",
    "for i in ar.index:\n",
    "    proc_year=\"none\"\n",
    "    year_temp=re.sub(\"I\", \"1\", ar.loc[i,\"year_proc\"]) # sub for I issue\n",
    "    try:\n",
    "        proc_year=int(float(year_temp))  #convert from float to int\n",
    "        if (proc_year==0):\n",
    "            ar.loc[i, 'year_proc']='none'\n",
    "        elif (proc_year<1000)|(proc_year>2000):\n",
    "            # print(proc_year)\n",
    "            ar.loc[i, 'year_proc']='wrong'\n",
    "            continue\n",
    "        ar.loc[i, 'year_proc']=proc_year\n",
    "    except:\n",
    "        pst=0\n",
    "        if (year_temp in year_rec_split_out.keys()):\n",
    "            ar.loc[i, 'year_proc']=year_rec_split_out[year_temp]\n",
    "            pst=1\n",
    "        elif (\"-\" in year_temp) or (\"/\" in year_temp):\n",
    "            split_year=re.sub(\"/\", \"-\", year_temp).split('-')\n",
    "            if (len(split_year[0])==4) and (len(split_year[1])==4):\n",
    "                ar.loc[i, 'year_proc']=re.sub(\"/\", \"-\", year_temp)\n",
    "                pst=1\n",
    "        if (ar.loc[i, \"year_proc\"]!=\"none\") & (pst==0):\n",
    "            year_corr.append(year_temp) #append to list if could not directly convert\n",
    "\n",
    "\n",
    "year_corr_u=list(set(year_corr)) #get unique list\n",
    "year_corr_u.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d2c8fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in year_corr_u: #because all years have been resolved the year_corr is empty\n",
    "#     print('\"'+i+'\",')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31df4e1",
   "metadata": {},
   "source": [
    "Some year fields are multiple years, eg:1954-1955 in which case I take the latest year to occur. in the masterlist data does not have any entries with multiple years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d40fcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1969, 1964, 1968, 1945, 1951, 1953, 1957, 1965, 1967, 1966, 1926,\n",
       "       1962, 1959, 1958, 1925, 1954, 1948, 1955,    0, 1946, 1942, 1949,\n",
       "       1956, 1924, 1886, 1937, 1950, 1938, 1947, 1936, 1934, 1944, 1913,\n",
       "       1935, 1941, 1961, 1940, 1960, 1927, 1928, 1930, 1933, 1952, 1943,\n",
       "       1932, 1656, 1920, 1845, 1921, 1939, 1911, 1888, 1895, 1903, 1892,\n",
       "       1869, 1834, 1929, 1914, 1917, 1963, 1918, 1970, 1931, 1919,    1,\n",
       "       1908, 1923, 1916, 1922, 1907, 1912, 1906, 1909, 1897, 1870, 1910,\n",
       "       1915, 1887, 1605, 1868, 1900, 1902, 1879, 1843, 1853, 1832, 1860,\n",
       "       1844, 1848, 1891, 1890, 1893, 1871, 1901, 1864, 1863, 1899, 1904,\n",
       "       1862, 1858, 1880, 1794, 1796, 1795, 1778, 1883, 1874, 1803, 1810,\n",
       "       1872, 1819, 1821, 1823, 1831, 1812, 1846, 1809, 1875, 1811, 1801,\n",
       "       1807, 1826, 1841, 1838, 1847, 1837, 1877, 1839, 1804, 1830, 1840,\n",
       "       1824, 1859, 1772, 1836, 1856, 1878,    4, 1896,    3, 1898, 1852,\n",
       "       1971, 1905, 1849, 1851, 1873, 1882, 1885, 1884, 1889, 1861, 1867,\n",
       "       1894, 1685, 1695, 1782, 1788, 1490, 1550, 1569, 1546, 1544, 1553,\n",
       "       1592, 1556, 1596, 1642, 1681, 1652, 1761, 1744, 1743, 1835, 1664,\n",
       "       1582, 1581, 1601, 1790, 1763, 1768, 1767, 1736, 1793, 1771, 1754,\n",
       "       1760, 1781, 1756, 1789, 1766, 1765, 1764, 1769, 1705, 1777, 1993,\n",
       "       1876, 1984, 1881, 1588, 1606, 1850, 1720, 1682, 1725, 1775, 1857,\n",
       "       1702, 1785, 1855, 1833, 1690, 1676, 1696, 1721, 1774, 1755, 1753,\n",
       "       1797, 1724, 1732, 1740, 1719, 1776, 1787, 1791, 1780, 1802, 1817,\n",
       "       1805, 1825, 1738, 1784, 1792, 1799, 1808, 1818, 1397, 1673, 1757,\n",
       "       1786, 1773, 1415, 1866, 1854, 1865, 1829, 1737, 1814, 1816, 1827,\n",
       "       1820, 1815, 1973, 1972, 1975, 1974, 1663, 1633, 1742, 1798, 1542,\n",
       "       1618, 1629, 1988, 1746, 1684, 1726, 1691, 1750, 1752, 1729, 1770,\n",
       "       1739, 1758, 1762, 1976, 1646, 1713, 1728, 1748, 1994, 1679, 1842,\n",
       "       1406, 1813, 1707, 1693, 1623, 1622, 1638, 1600, 1731, 1985, 1422,\n",
       "       1987, 1699, 1822, 1783, 1800, 1195, 1751, 1734, 1714, 1986, 1806,\n",
       "       1978, 1389, 1828, 1717, 1674, 1604, 1607, 1680, 1660, 1759, 1997,\n",
       "       1073, 1024, 1297, 1080, 1476, 1202, 1123, 1979, 1983, 1981, 1779,\n",
       "       1620, 1668, 1697, 1687, 1360, 1591, 1564, 1332, 1578, 1648, 1669,\n",
       "       1723, 1990, 1300])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def year_split(x):\n",
    "    if x[0]==\"none\":\n",
    "        return 0\n",
    "    if x[0]==\"wrong\":\n",
    "        return 1\n",
    "    if x[0]==\"forthcoming\":\n",
    "        return 3\n",
    "    if x[0]==\"uncertain\":\n",
    "        return 4\n",
    "    if x[0]==\"various\":\n",
    "        return 4\n",
    "    if len(x)==1:\n",
    "        return int(x[0])\n",
    "    if len(x)==2:\n",
    "        return int(x[1])\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "ar[\"year_proc_split\"]=ar[\"year_proc\"].astype(str)\n",
    "ar[\"year_proc_split\"]=ar[\"year_proc_split\"].str.split('-')\n",
    "ar[\"year_latest\"]=ar[\"year_proc_split\"].apply(lambda x: year_split(x))\n",
    "ar[\"year_latest\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a76e2c9",
   "metadata": {},
   "source": [
    "## Volume reconcilliation\n",
    "\n",
    "1. fillna as \"none\" and convert the year column to type string, store it in a new column called volume_proc\n",
    "2. I expect volume to be in either roman numerals or an integer, functions below convert roman numerals to decimals, detect roman numerals and detect that a piece of text is only numbers.\n",
    "3. for each value, try caste the string to an int, either directly or first converted to a float\n",
    "4. append all other cases to a list called oc_u, get a unique set and reconcile manually via a list called vol_rec\n",
    "5. Format of errors is {correct_volume: [volume_error1, volume_error2 ...], ...}\n",
    "6. Save it to a file called volume_recon.json. this is the file that is being read after the next code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e69ed645",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_rec=None\n",
    "with open(\"./031_recon/mturk_volume_recon.json\", 'r') as f:\n",
    "    vol_rec = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "050d6b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_rec_split_out={}\n",
    "for key in vol_rec.keys():\n",
    "    for k in vol_rec[key]:\n",
    "        vol_rec_split_out[k]=key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "931f816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar[\"volume_proc\"]=ar[\"volume\"].fillna(\"none\").astype(str).str.upper()\n",
    "vol_out=list(ar[\"volume_proc\"].unique())\n",
    "vol_out.sort()\n",
    "\n",
    "w_count=0\n",
    "n_count=0\n",
    "vol_corr=[]\n",
    "\n",
    "for i in ar.index:\n",
    "    val=ar.loc[i,'volume_proc']\n",
    "    if val in vol_rec_split_out.keys():\n",
    "        val=vol_rec_split_out[val]\n",
    "    if number_match(val):\n",
    "        ar.loc[i,'volume_proc']=int(float(val))\n",
    "    elif rom_match(val):\n",
    "        ar.loc[i,'volume_proc']=rom_to_dec(val)\n",
    "    elif val==\"NONE\":\n",
    "        n_count=n_count+1\n",
    "    elif val in vol_rec_split_out.keys():\n",
    "        ar.loc[i,'volume_proc']=vol_rec_split_out[val]\n",
    "        #print(val+\" \"+vol_rec_split_out[val])\n",
    "    else:\n",
    "        w_count=w_count+1\n",
    "        vol_corr.append(val)\n",
    "        print('\"'+val+'\",')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69fde836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE THE output is empty because all issues have been resolved\n",
    "vol_corr_u=list(set(vol_corr))\n",
    "vol_corr_u.sort()\n",
    "for a in vol_corr_u:\n",
    "    print('\"'+a+'\":[\\\"'+a+'\\\"],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21f05919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/Users/sijiawu/Work/Refs Danae/volume_recon.json\", 'w') as f:\n",
    "#     json.dump(vol_rec, f, indent = 6) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf454a0",
   "metadata": {},
   "source": [
    "## Issue reconcilliation\n",
    "\n",
    "1. fillna as \"none\" and convert the year column to type string, store it in a new column called volume_proc\n",
    "2. I expect issue to be in either roman numerals or an integer, or a float. This part uses the same functions as the previous section to for converting roman numerals to decimals, detect roman numerals and detect that a piece of text is only numbers.\n",
    "3. for each value, I fist check if the value is to be corrected against the compiled file. Then try caste the string value of the issue to an int if it is a number, either directly or first converted to a float. if not a number then check for a roman numeral and then convert that to an integer. If it fails all the previous conditions check if None or if it is a string value designated to be so in the corrections file.\n",
    "4. append all other cases to a list called issue_corr, get a unique set and reconcile manually via a list called issue_rec which is saved in the file issue_recon.json\n",
    "5. Format of errors is {correct_issue: [issue_error1, issue_error2 ...], ...}\n",
    "6. Iteratively perform the above until all errors are resolved. Save it to a file called issue_recon.json. this is the file that is being read in the next code block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a6451da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_rec=None\n",
    "with open(\"./031_recon/mturk_issue_recon.json\", 'r') as f:\n",
    "    issue_rec=json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "21aeae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make key-value pairs that can easily replace things.\n",
    "issue_rec_split_out={}\n",
    "for key in issue_rec.keys():\n",
    "    for k in issue_rec[key]:\n",
    "        issue_rec_split_out[k]=key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "374dc062",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar[\"issue_proc\"]=ar[\"issue\"].fillna(\"none\").astype(str).str.upper()\n",
    "issue_out=list(ar[\"issue_proc\"].unique())\n",
    "issue_out.sort()\n",
    "\n",
    "w_count=0\n",
    "n_count=0\n",
    "issue_corr=[]\n",
    "\n",
    "for i in ar.index:\n",
    "    val=ar.loc[i,'issue_proc']\n",
    "    ind=0\n",
    "    if val in issue_rec_split_out.keys():\n",
    "        val=issue_rec_split_out[val]\n",
    "    if number_match(val):\n",
    "        ar.loc[i,'issue_proc']=int(float(val))\n",
    "    elif rom_match(val):\n",
    "        ar.loc[i,'issue_proc']=rom_to_dec(val)\n",
    "    elif val==\"NONE\":\n",
    "        n_count=n_count+1\n",
    "        ar.loc[i,'issue_proc']=val\n",
    "    elif val in issue_rec_split_out.keys():\n",
    "        ar.loc[i,'issue_proc']=val\n",
    "#         print(val)\n",
    "    else:\n",
    "        w_count=w_count+1\n",
    "        issue_corr.append(val)\n",
    "        # print('\"'+str(val)+'\"')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed59ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c0f9ea2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1,2,54\",\n",
      "\"166-69\",\n",
      "\"2D SERIES\",\n",
      "\"7-9\",\n",
      "\"FASC. 1\",\n",
      "\"FASC. 2A\",\n",
      "\"FASC. 3\",\n",
      "\"FASC. 4A\",\n",
      "\"FIRST SEMESTER\",\n",
      "\"SONDERHEFT 41\",\n",
      "\"SUPPLEMENT IX\",\n"
     ]
    }
   ],
   "source": [
    "issue_corr_u=list(set(issue_corr))\n",
    "issue_corr_u.sort()\n",
    "for a in issue_corr_u:\n",
    "    print('\"'+a+'\",')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a290610",
   "metadata": {},
   "source": [
    "## Titles\n",
    "\n",
    "For titles, I only compile corrections for those of journal articles. \n",
    "1. lower the test and fill the na values with \"none\". Strip leading and trailing characters that don't belong in titles. Assign these titles to a new column: \n",
    "2. replace americanized spelling, replace characters that don't belong in titles\n",
    "3. 46989 total references with 26603 unique titles. There are 12561 journal article references of which 9913 are unique. Since I only care for those in the top 5 econ journals. There are 5074 top 5 journal references of which 3172 are unique titles. After cleaning for duplicates and spelling mistakes, unique top 5 article titles reduce to 2822 unique titles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "160bca01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    13723\n",
       "2    12482\n",
       "4    11788\n",
       "1     6566\n",
       "8     1016\n",
       "7      856\n",
       "6      320\n",
       "5       23\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f5d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "70c47f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22930"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pre-process the title with previous two functions\n",
    "ar[\"title_proc\"]=ar[\"title\"].fillna(\"none\").astype(str).str.lower()\n",
    "ar[\"title_proc\"]=ar[\"title_proc\"].apply(strip_leading,1)\n",
    "ar[\"title_proc\"]=ar[\"title_proc\"].apply(replace_d_space,1)\n",
    "title_sort=list(ar.title_proc.unique())\n",
    "title_sort.sort()\n",
    "\n",
    "j_data[\"title_proc\"]=j_data[\"title\"].fillna(\"none\").astype(str).str.lower()\n",
    "j_data[\"title_proc\"]=j_data[\"title_proc\"].apply(strip_leading,1)\n",
    "\n",
    "len(title_sort) #total titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ae04a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5039\n",
      "3156\n"
     ]
    }
   ],
   "source": [
    "print(len(ar[(ar['type']==2) & (ar['journal_proc'].isin(j_list_app)==True)][\"title_proc\"]))\n",
    "print(len(ar[(ar['type']==2) & (ar['journal_proc'].isin(j_list_app)==True)][\"title_proc\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5ad828ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the title recon file\n",
    "title_rec=None\n",
    "with open(\"./031_recon/mturk_title_recon.json\", 'r') as f:\n",
    "    title_rec=json.load(f) \n",
    "\n",
    "#reformat and expand the title recon file\n",
    "title_rec_split_out={}\n",
    "for key in title_rec.keys():\n",
    "    for k in title_rec[key]:\n",
    "        title_rec_split_out[k]=key  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d5daeacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ar.index:\n",
    "    if ar.loc[i,\"title_proc\"] in title_rec_split_out.keys():\n",
    "        ar.loc[i,\"title_proc\"]=title_rec_split_out[ar.loc[i,\"title_proc\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d885180",
   "metadata": {},
   "source": [
    "## Merge two data sets\n",
    "Merge the fMturk and Mturk origin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d992d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=rem_refs.columns\n",
    "t2=ar.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "76c1f825",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullset=pd.concat([ar,rem_refs]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2d58d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullset.to_pickle(\"pre_1970s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
